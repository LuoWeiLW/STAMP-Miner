# -*- coding: utf-8 -*-
"""
STAMP-Miner Module 3: Targeted Recognition Validation (Inference)
Function: Load trained AWLSTM model and predict specificity against target pathogens.
"""

import os
import re
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
from pathlib import Path
from torch.utils.data import DataLoader
from torchtext.vocab import vocab as torch_vocab
from collections import OrderedDict

# --- ç¯å¢ƒé…ç½® ---
def get_args():
    parser = argparse.ArgumentParser(description="STAMP-Miner AWLSTM Inference Pipeline")
    parser.add_argument('--input', type=str, default='results/04_docking_ifp/top100_observed_ifp.csv', 
                        help='Path to the input CSV file from Module 2')
    parser.add_argument('--model_path', type=str, default='bin/AWLSTM_2.pth', 
                        help='Path to the trained .pth model')
    parser.add_argument('--dict_path', type=str, default='bin/dict_AWLSTM.csv', 
                        help='Path to the dictionary file')
    parser.add_argument('--output', type=str, default='results/05_final_leads/P1_P4_candidates.csv', 
                        help='Path to save the prediction results')
    parser.add_argument('--max_len', type=int, default=70, help='Max sequence length for padding')
    return parser.parse_args()


def get_args():
    parser = argparse.ArgumentParser(description="STAMP-Miner AWLSTM Inference Pipeline")
    parser.add_argument('--input_csv', type=str, default='results/04_docking_ifp/top100_observed_ifp.csv', 
                        help='Path to the input CSV file from Module 2')
    
    parser.add_argument('--model_path', type=str, default='bin/HWLSTM.pth', 
                        help='Path to the trained .pth model')
    
    parser.add_argument('--dict_path', type=str, default='bin/dict_AWLSTM.csv', 
                        help='Path to the dictionary file')
    
    parser.add_argument('--output', type=str, default='results/05_final_leads/P1_P4_candidates.csv', 
                        help='Path to save the prediction results')
    parser.add_argument('--max_len', type=int, default=70, help='Max sequence length')
    return parser.parse_args()


# --- æ¨¡å‹å®šä¹‰ (ä¿æŒä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´) ---
class LSTM_Net(nn.Module):
    def __init__(self, vocab_size, embedding_dim=300, hidden_size=70, max_len=70):
        super(LSTM_Net, self).__init__()
        self.max_len = max_len
        self.em = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)
        self.fc1 = nn.Linear(hidden_size * max_len, 256)
        self.fc2 = nn.Linear(256, 64)
        self.fc3 = nn.Linear(64, 2)

    def forward(self, x):
        x = self.em(x)
        x, _ = self.lstm(x)
        x = x.contiguous().view(len(x), -1)
        x = F.dropout(F.relu(self.fc1(x)), p=0.8)
        x = F.dropout(F.relu(self.fc2(x)), p=0.4)
        x = self.fc3(x)
        return x

# --- æ•°æ®å¤„ç†å·¥å…· ---
def reg_text(sequence):
    """æå–æ°¨åŸºé…¸åºåˆ—ä¸­çš„å­—æ¯"""
    token = re.compile('[A-Za-z]')
    return token.findall(str(sequence))

def load_custom_vocab(dict_path):
    """ä»å¯¼å‡ºçš„CSVåŠ è½½è¯è¡¨ï¼Œç¡®ä¿ç´¢å¼•ä¸¥æ ¼ä¸€è‡´"""
    df_dict = pd.read_csv(dict_path)
    # å‡è®¾CSVç¬¬ä¸€è¡Œæ˜¯è¯è¡¨å­—å…¸ {å­—æ¯: ç´¢å¼•}
    stoi = df_dict.iloc[0].to_dict()
    # è½¬æ¢ç´¢å¼•ä¸ºæ•´æ•°
    stoi = {k: int(v) for k, v in stoi.items()}
    # æ„å»º torchtext å…¼å®¹çš„ vocab å¯¹è±¡
    sorted_dict = OrderedDict(sorted(stoi.items(), key=lambda v: v[1]))
    v = torch_vocab(sorted_dict)
    v.set_default_index(v["<unk>"] if "<unk>" in stoi else 0)
    return v

def main():
    args = get_args()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ Using device: {device}")

    # 1. åŠ è½½è¯è¡¨
    if not os.path.exists(args.dict_path):
        raise FileNotFoundError(f"âŒ Dictionary not found at {args.dict_path}. Please ensure it exists in bin/")
    vocab = load_custom_vocab(args.dict_path)
    vocab_size = len(vocab)
    print(f"ğŸ“š Vocab size loaded: {vocab_size}")

    # 2. åˆå§‹åŒ–å¹¶åŠ è½½æ¨¡å‹
    model = LSTM_Net(vocab_size, max_len=args.max_len).to(device)
    if not os.path.exists(args.model_path):
        raise FileNotFoundError(f"âŒ Model weights not found at {args.model_path}")
    
    # å…¼å®¹ä¸¤ç§ä¿å­˜æ–¹å¼ï¼šstate_dict æˆ– å®Œæ•´æ¨¡å‹
    try:
        model.load_state_dict(torch.load(args.model_path, map_location=device))
    except:
        model = torch.load(args.model_path, map_location=device)
    
    model.eval()
    print("ğŸ§  AWLSTM model loaded successfully.")

    # 3. è½½å…¥å¹¶é¢„å¤„ç†å¾…é¢„æµ‹æ•°æ®
    df = pd.read_csv(args.input)
    print(f"ğŸ“¥ Loading {len(df)} candidates from {args.input}")
    
    sequences = df['sequence'].apply(reg_text)
    
    # 4. åºåˆ—è½¬ä¸º Tensor å¹¶ Padding
    x_list = []
    for seq in sequences:
        indexed_seq = vocab(seq)
        # å›ºå®šé•¿åº¦å¤„ç† (Padding & Truncating)
        if len(indexed_seq) < args.max_len:
            indexed_seq = indexed_seq + [0] * (args.max_len - len(indexed_seq))
        else:
            indexed_seq = indexed_seq[:args.max_len]
        x_list.append(torch.tensor(indexed_seq, dtype=torch.int64))

    x_tensor = torch.stack(x_list).to(device)

    # 5. æ¨¡å‹é¢„æµ‹
    print("ğŸ§ª Running inference...")
    with torch.no_grad():
        logits = model(x_tensor)
        probs = F.softmax(logits, dim=1)
        preds = torch.argmax(logits, dim=1).cpu().numpy()
        amp_probs = probs[:, 1].cpu().numpy() # è·å–å±äºAMPç±»åˆ«çš„æ¦‚ç‡

    # 6. ä¿å­˜ç»“æœ
    df['AWLSTM_prediction'] = preds
    df['AMP_probability'] = np.round(amp_probs, 4)
    
    # ç­›é€‰é¢„æµ‹ä¸ºæ­£æ ·æœ¬çš„å€™é€‰è‚½ (P1-P4 ä¼˜é€‰)
    os.makedirs(os.path.dirname(args.output), exist_ok=True)
    df.to_csv(args.output, index=False)
    
    pos_count = np.sum(preds)
    print(f"âœ… Prediction finished. Found {pos_count} potential STAMPs.")
    print(f"ğŸ’¾ Results saved to: {args.output}")

if __name__ == "__main__":
    main()
